# Neurological Assessment Analysis Pipeline

This project provides a multi-step pipeline for processing and analyzing video recordings from a computerized neurological assessment. The pipeline automates perspective correction, ball and gaze tracking, video segmentation, and the generation of a detailed performance report.

## Pipeline Workflow

The process is divided into three main scripts that must be run in sequence:

1.  `detect_and_save_ball.py`: Processes the raw video to correct perspective, detect the ball, and overlay gaze data.
2.  `trim_video.py`: Segments the processed video into "fast" and "slow" phases based on a visual cue.
3.  `generate_report.py`: Analyzes the segmented data to produce a final report with metrics and visualizations.

## Prerequisites

Before running the scripts, ensure you have Python installed along with the following libraries. You can install them using pip:

```bash
pip install pandas numpy opencv-python ultralytics easyocr matplotlib seaborn openpyxl
```

For better performance, it is recommended to have a CUDA-compatible GPU for `ultralytics` (YOLO) and `easyocr`.

## Usage

Place all your input files (videos and `.csv` data from the eye-tracker) in the same directory as the scripts.

### Step 1: Ball Detection and Gaze Analysis

This script takes the raw experiment video and associated data files, performs perspective correction, detects a moving ball, overlays the user's gaze, and saves the processed video and a corresponding analysis data file.

**Purpose:**
*   Aligns video frames with gaze data using timestamps.
*   Reads surface corner coordinates from `surface_positions.csv` to perform a perspective warp, "straightening" the view for each frame.
*   Detects the ball in the warped video using either the YOLOv8 model or OpenCV's Hough Circle Transform.
*   Overlays the user's gaze position onto the warped video.
*   Calculates whether the gaze is inside an enlarged bounding box around the ball.
*   Saves the processed video and a detailed CSV file (`_analysis.csv`) containing frame-by-frame data on ball position, gaze position, and gaze-in-box status.

**Inputs:**
*   `video.mp4`: The raw video from the eye-tracker.
*   `world_timestamps.csv`: Timestamps for each video frame.
*   `gaze.csv`: Gaze position data.
*   `surface_positions.csv`: The coordinates of the four corners of the target surface for each frame.
*   `3d_eye_states.csv`: Pupillometry data.
*   (Optional) `yolov8x.pt`: The YOLOv8 model file if using the YOLO detector.

**Command:**

```bash
# To use the Hough Circle Transform detector (default)
python detect_and_save_ball.py --input_video video.mp4 --gaze_csv gaze.csv --world_timestamps world_timestamps.csv --surface_positions surface_positions.csv

# To use the more accurate YOLOv8 detector
python detect_and_save_ball.py --use_yolo --input_video video.mp4 --gaze_csv gaze.csv --world_timestamps world_timestamps.csv --surface_positions surface_positions.csv
```

**Outputs:**
*   `output_final_analysis.mp4`: The perspective-corrected video with the ball's bounding box and gaze point drawn on it.
*   `output_final_analysis_analysis.csv`: A CSV file with detailed metrics for each processed frame.

---

### Step 2: Video Trimming by Visual Cue

This script analyzes the video generated in Step 1 to find a specific reference point (`t0`) and then cuts the video into two distinct segments: "fast" and "slow".

**Purpose:**
*   Scans the input video frame-by-frame using EasyOCR to find a visual cue (the number '1').
*   Establishes a reference time `t0` when the number '1' disappears after being stably detected.
*   Calculates start and end frames for a "fast" and a "slow" segment based on fixed durations relative to `t0`.
*   Saves the frame numbers for these segments into `cut_points.csv`.
*   Saves the two video segments as separate files.

**Inputs:**
*   `output_final_analysis.mp4`: The video generated by `detect_and_save_ball.py`.

**Command:**

```bash
python trim_video.py --input_video output_final_analysis.mp4
```

**Outputs:**
*   `trimmed_video_fast.mp4`: The video segment for the "fast" phase.
*   `trimmed_video_slow.mp4`: The video segment for the "slow" phase.
*   `cut_points.csv`: A file containing the start and end frames for the `fast` and `slow` segments.

---

### Step 3: Report Generation

This final script aggregates all the generated data to produce a comprehensive report in Excel format, along with several data visualizations.

**Purpose:**
*   Loads the analysis data from Step 1 (`output_final_analysis_analysis.csv`) and the segment definitions from Step 2 (`cut_points.csv`).
*   Aligns and integrates pupillometry data from `3d_eye_states.csv`.
*   For each segment ("fast" and "slow"):
    *   Segments the data into individual ball movements (up, down, left, right).
    *   Calculates key metrics for each movement: ball speed, gaze latency, and average pupil dilation.
    *   Generates summary statistics for each movement direction.
    *   Creates and saves visualizations: gaze heatmaps and a plot showing the average pupil dilation trend.
*   Saves all summary and detailed metrics into a multi-sheet Excel file.

**Inputs:**
*   `output_final_analysis_analysis.csv`: The analysis data from Step 1.
*   `cut_points.csv`: The segment definitions from Step 2.
*   `3d_eye_states.csv`: Raw pupillometry data.
*   `world_timestamps.csv`: For aligning pupil data.
*   `output_final_analysis.mp4`: To get video dimensions for plotting.

**Command:**

```bash
# Ensure all input files are in the same directory
python generate_report.py
```

**Outputs:**
*   `final_report.xlsx`: A multi-sheet Excel file with summary and detailed metrics for both "fast" and "slow" segments.
*   `report_plots/` (directory): Contains all generated plots and heatmaps as PNG images.

## Citation

If you use this script in your research or work, please cite the following publications:

Lozzi, D.; Di Pompeo, I.; Marcaccio, M.; Ademaj, M.; Migliore, S.; Curcio, G. SPEED: A Graphical User Interface Software for Processing Eye Tracking Data. NeuroSci 2025, 6, 35. https://doi.org/10.3390/neurosci6020035

Lozzi, D.; Di Pompeo, I.; Marcaccio, M.; Alemanno, M.; Kr√ºger, M.; Curcio, G.; Migliore, S. AI-Powered Analysis of Eye Tracker Data in Basketball Game. Sensors 2025, 25, 3572. https://doi.org/10.3390/s25113572